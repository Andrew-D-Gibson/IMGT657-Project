{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6139abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from TicTacToe import TicTacToe\n",
    "from MCTS import MCTS\n",
    "from simulate_self_play_games import simulate_self_play_games\n",
    "from head_to_head_match import head_to_head_match, head_to_head_game\n",
    "from TrainingManager import TrainingManager\n",
    "from PureNetworkAgent import PureNetworkAgent\n",
    "\n",
    "from Config import *\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------- Episode  0  ---------\n",
      "Simulating games:\n",
      "1/18, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/18, 3/18, 4/18, 5/18, 6/18, 7/18, 8/18, 9/18, 10/18, 11/18, 12/18, 13/18, 14/18, 15/18, 16/18, 17/18, 18/18, 132 examples generated, 132 total.\n",
      "\n",
      "Training on examples:\n",
      "1/1-132,  Minibatch Policy Acc: 0.8484848737716675, Value Acc: 0.8939393758773804\n",
      "New network against old network (testing for improvement): \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 2 / 4 / 0\n",
      "Replacing network!  Great success.\n",
      "INFO:tensorflow:Assets written to: Networks/Episode_0/assets\n",
      "MCTS with Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 6 / 0\n",
      "Raw Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 0 / 6\n",
      "Total episode time: 132.0 seconds\n",
      "\n",
      "\n",
      "--------- Episode  1  ---------\n",
      "Simulating games:\n",
      "1/18, 2/18, 3/18, 4/18, 5/18, 6/18, 7/18, 8/18, 9/18, 10/18, 11/18, 12/18, 13/18, 14/18, 15/18, 16/18, 17/18, 18/18, 138 examples generated, 270 total.\n",
      "\n",
      "Training on examples:\n",
      "1/1-270,  Minibatch Policy Acc: 0.8481481671333313, Value Acc: 0.8370370268821716\n",
      "New network against old network (testing for improvement): \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 2 / 4 / 0\n",
      "Replacing network!  Great success.\n",
      "INFO:tensorflow:Assets written to: Networks/Episode_1/assets\n",
      "MCTS with Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 6 / 0\n",
      "Raw Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 4 / 2\n",
      "Total episode time: 133.8 seconds\n",
      "\n",
      "\n",
      "--------- Episode  2  ---------\n",
      "Simulating games:\n",
      "1/18, 2/18, 3/18, 4/18, 5/18, 6/18, 7/18, 8/18, 9/18, 10/18, 11/18, 12/18, 13/18, 14/18, 15/18, 16/18, 17/18, 18/18, 154 examples generated, 424 total.\n",
      "\n",
      "Training on examples:\n",
      "1/1-424,  Minibatch Policy Acc: 0.8207547068595886, Value Acc: 0.8773584961891174\n",
      "New network against old network (testing for improvement): \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 2 / 4 / 0\n",
      "Replacing network!  Great success.\n",
      "INFO:tensorflow:Assets written to: Networks/Episode_2/assets\n",
      "MCTS with Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 6 / 0\n",
      "Raw Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 4 / 2\n",
      "Total episode time: 142.8 seconds\n",
      "\n",
      "\n",
      "--------- Episode  3  ---------\n",
      "Simulating games:\n",
      "1/18, 2/18, 3/18, 4/18, 5/18, 6/18, 7/18, 8/18, 9/18, 10/18, 11/18, 12/18, 13/18, 14/18, 15/18, 16/18, 17/18, 18/18, 158 examples generated, 582 total.\n",
      "\n",
      "Training on examples:\n",
      "1/1-582,  Minibatch Policy Acc: 0.8556700944900513, Value Acc: 0.907216489315033\n",
      "New network against old network (testing for improvement): \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 2 / 3 / 1\n",
      "Replacing network!  Great success.\n",
      "INFO:tensorflow:Assets written to: Networks/Episode_3/assets\n",
      "MCTS with Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 6 / 0\n",
      "Raw Best network against raw MCTS: \n",
      "1/6, 2/6, 3/6, 4/6, 5/6, 6/6, W/D/L: 0 / 6 / 0\n",
      "Total episode time: 143.8 seconds\n",
      "\n",
      "\n",
      "--------- Episode  4  ---------\n",
      "Simulating games:\n",
      "1/18, 2/18, 3/18, 4/18, 5/18, 6/18, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m manager \u001b[39m=\u001b[39m TrainingManager()\n\u001b[0;32m----> 2\u001b[0m manager\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/TrainingManager.py:85\u001b[0m, in \u001b[0;36mTrainingManager.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m# Simulate games\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSimulating games:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m new_examples \u001b[39m=\u001b[39m simulate_self_play_games(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_model)\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_examples\u001b[39m.\u001b[39mextend(new_examples)\n\u001b[1;32m     87\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(new_examples)\u001b[39m}\u001b[39;00m\u001b[39m examples generated, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_examples)\u001b[39m}\u001b[39;00m\u001b[39m total.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/simulate_self_play_games.py:41\u001b[0m, in \u001b[0;36msimulate_self_play_games\u001b[0;34m(keras_model)\u001b[0m\n\u001b[1;32m     38\u001b[0m     mcts \u001b[39m=\u001b[39m MCTS(keras_model\u001b[39m=\u001b[39mkeras_model)\n\u001b[1;32m     40\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     mcts\u001b[39m.\u001b[39;49msearch()\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Pick the next node using the search probabilities\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     child_choice \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(mcts\u001b[39m.\u001b[39mpi), p\u001b[39m=\u001b[39mmcts\u001b[39m.\u001b[39mpi)\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/MCTS.py:109\u001b[0m, in \u001b[0;36mMCTS.search\u001b[0;34m(self, depth_to_search, seconds_to_search)\u001b[0m\n\u001b[1;32m    107\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    108\u001b[0m     \u001b[39mwhile\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m<\u001b[39m seconds_to_search:\n\u001b[0;32m--> 109\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMCTS_iteration()\n\u001b[1;32m    112\u001b[0m \u001b[39m# Find the number of visits (n) for each node as a proxy for goodness of the node\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# (Not the highest average value, to make sure we're confident)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m children_n \u001b[39m=\u001b[39m [child\u001b[39m.\u001b[39mn \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren]\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/MCTS.py:169\u001b[0m, in \u001b[0;36mMCTS.MCTS_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The -value_term is because this node is its child node's opponent\u001b[39;00m\n\u001b[1;32m    167\u001b[0m best_UCB \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(children_UCB)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchildren[best_UCB]\u001b[39m.\u001b[39;49mMCTS_iteration()\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/MCTS.py:169\u001b[0m, in \u001b[0;36mMCTS.MCTS_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The -value_term is because this node is its child node's opponent\u001b[39;00m\n\u001b[1;32m    167\u001b[0m best_UCB \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(children_UCB)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchildren[best_UCB]\u001b[39m.\u001b[39;49mMCTS_iteration()\n",
      "    \u001b[0;31m[... skipping similar frames: MCTS.MCTS_iteration at line 169 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/MCTS.py:169\u001b[0m, in \u001b[0;36mMCTS.MCTS_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The -value_term is because this node is its child node's opponent\u001b[39;00m\n\u001b[1;32m    167\u001b[0m best_UCB \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(children_UCB)\n\u001b[0;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchildren[best_UCB]\u001b[39m.\u001b[39;49mMCTS_iteration()\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/MCTS.py:140\u001b[0m, in \u001b[0;36mMCTS.MCTS_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMCTS_iteration\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39m# Check if we're at a terminal node (no more game to play because it's over!)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# If so, then just backpropagate the value of the position\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     game_over, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboard\u001b[39m.\u001b[39;49mis_game_over()\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m game_over:\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackpropagate(\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mabs(value))    \u001b[39m# Prior node won or drew, so reward it with +1 (or 0 so w/e)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/IMGT657-Project/TicTacToe.py:61\u001b[0m, in \u001b[0;36mTicTacToe.is_game_over\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m win_combos \u001b[39m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[39m0b111000000\u001b[39m, \u001b[39m# Bottom row win\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39m0b000111000\u001b[39m, \u001b[39m# Middle row win\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39m0b001010100\u001b[39m  \u001b[39m# Top right to bottom left diagonal win\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ]\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m combo \u001b[39min\u001b[39;00m win_combos:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx \u001b[39m&\u001b[39m combo \u001b[39m==\u001b[39m combo:\n\u001b[1;32m     62\u001b[0m         \u001b[39m# X won\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo \u001b[39m&\u001b[39m combo \u001b[39m==\u001b[39m combo:\n\u001b[1;32m     65\u001b[0m         \u001b[39m# O won\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "manager = TrainingManager()\n",
    "manager.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
